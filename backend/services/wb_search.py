import aiohttp
import asyncio
import logging
import random
import json
from urllib.parse import quote
from typing import Dict, Any

# –õ–æ–≥–≥–µ—Ä –≤ –∫–æ–Ω—Å–æ–ª—å, —á—Ç–æ–±—ã –≤—ã –≤–∏–¥–µ–ª–∏ –≤—Å—ë
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("WBSearch")

GEO_ZONES = {
    "moscow": "-1257786",      
    "spb": "-1257786",         
    "kazan": "-2133464",       
    "krasnodar": "-1192533",   
    "ekb": "-1113276",         
    "novosibirsk": "-1282245", 
    "khabarovsk": "-1216606",
    "belarus": "1235",         
    "kazakhstan": "-1227092"
}

# –ò–º–∏—Ç–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ Android —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
USER_AGENTS = [
    "Mozilla/5.0 (Linux; Android 13; SM-S918B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 12; Pixel 6 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 11; SM-A525F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Mobile Safari/537.36"
]

class WBSearchService:
    # –û–ë–ù–û–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º v9 (—Å–∞–º—ã–π —Å–≤–µ–∂–∏–π)
    BASE_URL = "https://search.wb.ru/exactmatch/ru/common/v9/search"

    def _get_headers(self):
        return {
            "User-Agent": random.choice(USER_AGENTS),
            "Accept": "*/*",
            "Accept-Encoding": "gzip, deflate, br",
            "Origin": "https://www.wildberries.ru",
            "x-requested-with": "XMLHttpRequest"
        }

    async def get_sku_position(self, query: str, target_sku: int, geo: str = "moscow", depth_pages: int = 5) -> Dict[str, Any]:
        dest_id = GEO_ZONES.get(geo, GEO_ZONES["moscow"])
        encoded_query = quote(query)
        target_sku = int(target_sku)
        
        logger.info(f"üîé [SEARCH v9] –ò—â—É SKU {target_sku} –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}' –≤ —Ä–µ–≥–∏–æ–Ω–µ {geo}")

        result = {
            "sku": target_sku,
            "query": query,
            "geo": geo,
            "found": False,
            "page": None,
            "position": None,
            "absolute_pos": None,
            "is_advertising": False,
            "cpm": None,
            "top_3_found": [], # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏: —á—Ç–æ –±–æ—Ç –≤–∏–¥–∏—Ç –≤ —Ç–æ–ø–µ
            "total_products": 0
        }

        async with aiohttp.ClientSession(headers=self._get_headers()) as session:
            tasks = []
            for page in range(1, depth_pages + 1):
                # –û–ë–ù–û–í–õ–ï–ù–ò–ï: appType=64 (Android), sort=popular
                url = (
                    f"{self.BASE_URL}?"
                    f"ab_testing=false&appType=64&curr=rub&dest={dest_id}"
                    f"&query={encoded_query}&resultset=catalog&sort=popular"
                    f"&spp=30&suppressSpellcheck=false&page={page}"
                )
                tasks.append(self._fetch_page(session, url, page))
            
            pages_data = await asyncio.gather(*tasks)

        global_counter = 0
        sorted_pages = sorted(pages_data, key=lambda x: x['page'])
        
        for p_data in sorted_pages:
            products = p_data['products']
            
            # –õ–û–ì–ò–†–û–í–ê–ù–ò–ï: –ü–∏—à–µ–º –≤ –∫–æ–Ω—Å–æ–ª—å, —á—Ç–æ –º—ã –Ω–∞—à–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            if not products:
                logger.warning(f"‚ö†Ô∏è –°—Ç—Ä–∞–Ω–∏—Ü–∞ {p_data['page']}: –ü—É—Å—Ç–æ (0 —Ç–æ–≤–∞—Ä–æ–≤). –°—Ç–∞—Ç—É—Å: {p_data['status']}")
                continue
            else:
                first_item = products[0].get('name', 'Unknown')
                logger.info(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {p_data['page']}: –ù–∞–π–¥–µ–Ω–æ {len(products)} —Ç–æ–≤–∞—Ä–æ–≤. –¢–æ–ø-1: {first_item}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¢–æ–ø-3 –≤—Å–µ–π –≤—ã–¥–∞—á–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if p_data['page'] == 1:
                result['total_products'] = p_data['total']
                for i in range(min(3, len(products))):
                    result['top_3_found'].append({
                        "id": products[i].get('id'),
                        "name": products[i].get('name'),
                        "brand": products[i].get('brand')
                    })

            for idx, prod in enumerate(products):
                global_counter += 1
                
                # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ ID
                if prod.get('id') == target_sku:
                    logger.info(f"üéâ –ù–ê–®–ï–õ! {target_sku} –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {global_counter}")
                    
                    result['found'] = True
                    result['page'] = p_data['page']
                    result['position'] = idx + 1
                    result['absolute_pos'] = global_counter
                    
                    if prod.get('log'):
                        result['is_advertising'] = True
                        result['cpm'] = prod.get('log', {}).get('cpm')
                    
                    return result

        logger.info(f"üí® –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω. –ü—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ {global_counter} —Ç–æ–≤–∞—Ä–æ–≤. –ê—Ä—Ç–∏–∫—É–ª –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return result

    async def _fetch_page(self, session, url, page_num):
        try:
            async with session.get(url, timeout=8) as resp:
                if resp.status == 200:
                    try:
                        data = await resp.json()
                        return {
                            'page': page_num, 
                            'products': data.get('data', {}).get('products', []),
                            'total': data.get('data', {}).get('total', 0),
                            'status': 200
                        }
                    except:
                        return {'page': page_num, 'products': [], 'total': 0, 'status': 'JSON_ERR'}
                return {'page': page_num, 'products': [], 'total': 0, 'status': resp.status}
        except Exception as e:
            logger.error(f"Page {page_num} Error: {e}")
            return {'page': page_num, 'products': [], 'total': 0, 'status': 'CONN_ERR'}

wb_search_service = WBSearchService()